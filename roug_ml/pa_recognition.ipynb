{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from beartype.typing import List, Dict\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get current working directory\n",
    "import sys\n",
    "current_path = os.getcwd()\n",
    "sys.path.append(os.path.join(current_path, '..'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.compat.v2.__internal__' has no attribute 'register_load_context_function'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mroug_ml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01meval_utl\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m calc_loss_acc_val\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mroug_ml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmulticlass\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m compute_multiclass_confusion_matrix\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mroug_ml\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutl\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmlflow_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_best_run\n",
      "File \u001b[0;32m~/DiaHecDev/pa_recognition/roug_ml/../roug_ml/utl/evaluation/eval_utl.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m accuracy_score\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m confusion_matrix, f1_score\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/tensorflow/__init__.py:473\u001b[0m\n\u001b[1;32m    471\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(_current_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    472\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 473\u001b[0m     \u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    474\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/tensorflow/python/util/lazy_loader.py:41\u001b[0m, in \u001b[0;36mLazyLoader._load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03m\"\"\"Load the module and insert it into the parent's globals.\"\"\"\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Import the target module and insert it into the parent's namespace\u001b[39;00m\n\u001b[0;32m---> 41\u001b[0m module \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parent_module_globals[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_local_name] \u001b[38;5;241m=\u001b[39m module\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Emit a warning if one was specified\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/keras/__init__.py:24\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tf2\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribute\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m models\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minput_layer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Input\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequential\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/keras/models/__init__.py:18\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Keras models API.\"\"\"\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# pylint: disable=g-bad-import-order\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Functional\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequential\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraining\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/keras/engine/functional.py:23\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mitertools\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m backend\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdtensor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layout_map \u001b[38;5;28;01mas\u001b[39;00m layout_map_lib\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m base_layer\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/keras/backend/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"AUTOGENERATED. DO NOT EDIT.\"\"\"\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m experimental\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;28mabs\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;28mall\u001b[39m\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/keras/backend/experimental/__init__.py:3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"AUTOGENERATED. DO NOT EDIT.\"\"\"\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m disable_tf_random_generator\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m enable_tf_random_generator\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m is_tf_random_generator_enabled\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/keras/src/__init__.py:21\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Implementation of the Keras API, the high-level API of TensorFlow.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03mDetailed documentation and user guides are available at\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03m[keras.io](https://keras.io).\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m distribute\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m models\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minput_layer\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Input\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequential\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/keras/src/models/__init__.py:18\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright 2022 The TensorFlow Authors. All Rights Reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Licensed under the Apache License, Version 2.0 (the \"License\");\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# limitations under the License.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# ==============================================================================\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m\"\"\"Keras models API.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Functional\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequential\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraining\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Model\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/keras/src/engine/functional.py:33\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m input_spec\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m node \u001b[38;5;28;01mas\u001b[39;00m node_module\n\u001b[0;32m---> 33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m training \u001b[38;5;28;01mas\u001b[39;00m training_lib\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mengine\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m training_utils\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m serialization_lib\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/keras/src/engine/training.py:41\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m optimizer_v1\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pickle_utils\n\u001b[0;32m---> 41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m saving_api\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m saving_lib\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m serialization_lib\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/keras/src/saving/saving_api.py:25\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf_export\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras_export\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m saving_lib\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m save \u001b[38;5;28;01mas\u001b[39;00m legacy_sm_saving_lib\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m io_utils\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/keras/src/saving/legacy/save.py:27\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m serialization\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaved_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load \u001b[38;5;28;01mas\u001b[39;00m saved_model_load\n\u001b[0;32m---> 27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaved_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_context\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaved_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m save \u001b[38;5;28;01mas\u001b[39;00m saved_model_save\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msrc\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaving\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlegacy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msaved_model\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras_option_scope\n",
      "File \u001b[0;32m~/miniforge3/lib/python3.10/site-packages/keras/src/saving/legacy/saved_model/load_context.py:68\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[38;5;124;03m\"\"\"Returns whether under a load context.\"\"\"\u001b[39;00m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _load_context\u001b[38;5;241m.\u001b[39min_load_context()\n\u001b[0;32m---> 68\u001b[0m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__internal__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregister_load_context_function\u001b[49m(in_load_context)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow.compat.v2.__internal__' has no attribute 'register_load_context_function'"
     ]
    }
   ],
   "source": [
    "from roug_ml.utl.evaluation.eval_utl import calc_loss_acc_val\n",
    "from roug_ml.utl.evaluation.multiclass import compute_multiclass_confusion_matrix\n",
    "from roug_ml.utl.mlflow_utils import get_best_run\n",
    "from roug_ml.utl.paths_utl import create_dir\n",
    "from roug_ml.models.hyperoptimization import generate_param_grid_with_different_size_layers, \\\n",
    "    get_or_create_experiment, get_best_run_from_hyperoptim\n",
    "from roug_ml.utl.processing.filters import LowPassFilter\n",
    "from roug_ml.utl.processing.signal_processing import SignalProcessor\n",
    "from roug_ml.utl.transforms.features_management import FeatureFlattener\n",
    "from roug_ml.utl.set_seed import set_seed\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "from roug_ml.models.hyperoptimization import parallele_hyper_optim\n",
    "from roug_ml.utl.parameter_utils import restructure_dict\n",
    "\n",
    "from roug_ml.utl.etl.data_extraction import extract_activity_data_from_users\n",
    "from roug_ml.models.pipelines.pipelines import NNTorch\n",
    "\n",
    "from processing.dataset_processing import covert_3a_from_pandas_to_dict\n",
    "from roug_ml.configs.data_paths import M_HEALTH_PATH\n",
    "from roug_ml.configs.data_labels import M_HEALTH_ACTIVITIES_LABELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "loss_functions = {\n",
    "    \"categorical_crossentropy\": \"categorical_crossentropy\"\n",
    "}\n",
    "\n",
    "\n",
    "def filter_data_dict(data_dict: dict, subjects: list) -> dict:\n",
    "    \"\"\"\n",
    "    Filter the data_dict dictionary based on a specific set of subjects.\n",
    "    :param data_dict: The input dictionary.\n",
    "    :param subjects: The list of subjects to keep.\n",
    "    returns: The filtered dictionary.\n",
    "    \"\"\"\n",
    "    filtered_dict = {\n",
    "        key: [value[i] for i in range(len(data_dict['user'])) if data_dict['user'][i] in subjects]\n",
    "        for key, value in data_dict.items()}\n",
    "    return filtered_dict\n",
    "\n",
    "\n",
    "class MHealtAnalysis:\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"\n",
    "        Performs following analysis:\n",
    "            - Extract data\n",
    "            - Make plots for analysis\n",
    "        \"\"\"\n",
    "        self.list_of_positions = ['chest', 'ankle', 'right_arm']\n",
    "        # self.list_of_positions = ['right_arm']\n",
    "        self.number_of_point_per_activity = 511  # TODO: automatic\n",
    "\n",
    "        self.features_to_extract = ['mean', 'std', 'rms', 'max', 'min', 'var']\n",
    "\n",
    "        # Define the number of parallel workers\n",
    "        # Use all available CPU cores except one\n",
    "        self.num_workers = 1  # multiprocessing.cpu_count() - 1\n",
    "        self.framework = 'torch'\n",
    "        # self.framework = 'tf'\n",
    "\n",
    "        # model params:\n",
    "        self.nn_params_keys = [\"activations\", \"in_nn\", \"input_shape\", \"output_shape\"]\n",
    "        self.other_keys = [\"batch_size\", \"cost_function\", \"learning_rate\", \"metrics\", \"n_epochs\",\n",
    "                           \"nn_key\"]\n",
    "\n",
    "        self.results_path = os.path.join(M_HEALTH_PATH, '../models')\n",
    "        create_dir(self.results_path)\n",
    "\n",
    "        mlflow.set_tracking_uri(\"http://localhost:8000\")\n",
    "        self.mlflow_experiment_name = 'run_non_parallel_shu_5'\n",
    "        self.mlflow_experiment_id = get_or_create_experiment(self.mlflow_experiment_name)\n",
    "\n",
    "        self.re_optimize = True\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        # Load data\n",
    "        dataset = self.read_data(M_HEALTH_PATH)\n",
    "\n",
    "        # Process data to right format\n",
    "        dataset_dict_chest = covert_3a_from_pandas_to_dict(in_dataset=dataset,\n",
    "                                                           in_label_col=23,\n",
    "                                                           in_user_col=24,\n",
    "                                                           in_3a_col=[0, 1, 2])\n",
    "\n",
    "        dataset_dict_ankle = covert_3a_from_pandas_to_dict(in_dataset=dataset,\n",
    "                                                           in_label_col=23,\n",
    "                                                           in_user_col=24,\n",
    "                                                           in_3a_col=[5, 6, 7])\n",
    "\n",
    "        dataset_dict_right_arm = covert_3a_from_pandas_to_dict(in_dataset=dataset,\n",
    "                                                               in_label_col=23,\n",
    "                                                               in_user_col=24,\n",
    "                                                               in_3a_col=[14, 15, 16])\n",
    "\n",
    "        dataset_dict = {\n",
    "            'y_num': dataset_dict_chest['y_num'],\n",
    "            'y_onehot': dataset_dict_chest['y_onehot'],\n",
    "            'x_chest': dataset_dict_chest['x'],\n",
    "            'x_ankle': dataset_dict_ankle['x'],\n",
    "            'x_right_arm': dataset_dict_right_arm['x'],\n",
    "            'User': dataset_dict_chest['User']\n",
    "        }\n",
    "\n",
    "        # Create dataset\n",
    "        final_data_set = self.extract_data_during_activity(\n",
    "            in_num_of_users=10,\n",
    "            in_dataset_dict=dataset_dict,\n",
    "            in_accelerometer_position=self.list_of_positions)\n",
    "\n",
    "        data_set_train, data_set_test = self.split_data_for_train_test_portion_of_pa(\n",
    "            in_data=final_data_set, in_points_in_each_set=self.number_of_point_per_activity,\n",
    "            in_accelerometer_position=self.list_of_positions)\n",
    "\n",
    "        # Split by patients: in the original dataset we use\n",
    "        subjects_to_keep = ['subject' + str(i) for i in range(7)]\n",
    "        data_set_train = filter_data_dict(data_set_train, subjects_to_keep)\n",
    "\n",
    "        subjects_to_keep = ['subject' + str(i + 7) for i in range(3)]\n",
    "        data_set_test = filter_data_dict(data_set_test, subjects_to_keep)\n",
    "\n",
    "        key_position = 'x_' + 'right_arm'\n",
    "        # key_position = 'x_' + 'chest'\n",
    "        # key_position = 'x_' + 'ankle'\n",
    "\n",
    "        x_train, y_train_oh = self.create_x_y(data_set_train, in_key=key_position + '_train',\n",
    "                                              in_features_to_extract=self.features_to_extract,\n",
    "                                              sample_freq=50,\n",
    "                                              window_size=1,\n",
    "                                              cutoff_frequency=5\n",
    "                                              )\n",
    "\n",
    "        x_val, y_val_oh = self.create_x_y(data_set_test, in_key=key_position + '_test',\n",
    "                                          in_features_to_extract=self.features_to_extract,\n",
    "                                          sample_freq=50,\n",
    "                                          window_size=1,\n",
    "                                          cutoff_frequency=5)\n",
    "\n",
    "        # Reshape the input data to match the expected shape of the model\n",
    "        x_train = x_train.reshape(x_train.shape[0], x_train.shape[1], 1)\n",
    "        x_val = x_val.reshape(x_val.shape[0], x_val.shape[1], 1)\n",
    "\n",
    "        y = y_train_oh\n",
    "        y_val = y_val_oh\n",
    "\n",
    "        if self.re_optimize:\n",
    "            # Optimize and get best run\n",
    "            params = self.generate_params(x_train=x_train)\n",
    "            results = self.hyperoptimize(x_train, y, x_val, y_val, params)\n",
    "            best_params, best_val_accuracy, best_run_id = get_best_run_from_hyperoptim(results)\n",
    "        else:\n",
    "            # Get the best run from mlopt\n",
    "            mlflow.set_tracking_uri(\"http://localhost:8000\")\n",
    "            best_run_id, best_params = get_best_run(self.mlflow_experiment_name, \"val_accuracy\")\n",
    "\n",
    "            best_params = restructure_dict(best_params, self.nn_params_keys, self.other_keys,\n",
    "                                           in_from_mlflow=True)\n",
    "\n",
    "        # Load the best model\n",
    "        best_model = mlflow.pytorch.load_model(\"runs:/{}/models\".format(best_run_id))\n",
    "\n",
    "        pipeline_torch = Pipeline(steps=[('NN', NNTorch(**best_params))])\n",
    "        pipeline_torch.named_steps['NN'].nn_model = best_model\n",
    "        #\n",
    "        predictions = pipeline_torch.predict(x_val)\n",
    "        # Convert one-hot encoded targets to binary labels\n",
    "        y_val_binary = np.argmax(y_val, axis=1)\n",
    "        val_acc = calc_loss_acc_val(predictions, y_val_binary)\n",
    "        print(val_acc)\n",
    "        compute_multiclass_confusion_matrix(targets=y_val_binary,\n",
    "                                            outputs=predictions,\n",
    "                                            class_labels=M_HEALTH_ACTIVITIES_LABELS\n",
    "                                            )\n",
    "\n",
    "    def generate_params(self, x_train):\n",
    "        \"\"\"\n",
    "        This function generates the list of parameters for the hyperparameter optimization.\n",
    "\n",
    "        :return: list_params: list of dictionaries, where each dictionary contains a unique\n",
    "         combination of hyperparameters\n",
    "        \"\"\"\n",
    "        # Define hyperparameters\n",
    "        nn_key = ['CNN']\n",
    "        input_shape = [np.asarray(x_train).shape[1]]\n",
    "        output_shape = [13]\n",
    "        batch_size = [  # 32, 64,\n",
    "            128]\n",
    "        cost_function = [loss_functions['categorical_crossentropy']]\n",
    "        learning_rate = [  # 0.01,\n",
    "            0.001]\n",
    "        n_epochs = [  # 300, 150, 140, 130, 120,\n",
    "            100]\n",
    "        metrics = ['accuracy']\n",
    "        layer_sizes = [  # [200, 300], [200, 300, 100], [200, 300, 400, 100], [50, 300, 100, 50],\n",
    "            # [100, 200, 200, 100], [50, 100, 100, 100, 50],\n",
    "            [100, 200, 100, 200, 100]\n",
    "        ]\n",
    "        activations = [\n",
    "            # ['relu', 'relu'], ['relu', 'tanh'], ['relu', 'relu', 'relu'],\n",
    "            # ['tanh', 'tanh', 'tanh', 'tanh'], ['relu', 'relu', 'relu', 'relu'],\n",
    "            # ['relu', 'relu', 'relu', 'relu', 'tanh'],\n",
    "            ['relu', 'relu', 'relu', 'relu', 'relu']\n",
    "        ]\n",
    "        cnn_filters = [1, 3, 5, 10]\n",
    "\n",
    "        list_params = generate_param_grid_with_different_size_layers(nn_key, input_shape,\n",
    "                                                                     output_shape, batch_size,\n",
    "                                                                     cost_function, learning_rate,\n",
    "                                                                     n_epochs, metrics,\n",
    "                                                                     layer_sizes, activations,\n",
    "                                                                     cnn_filters)\n",
    "\n",
    "        list_params = [\n",
    "            restructure_dict(params, self.nn_params_keys, self.other_keys, in_from_mlflow=False) for\n",
    "            params in list_params]\n",
    "\n",
    "        return list_params\n",
    "\n",
    "    def hyperoptimize(self, x_train, y, x_val, y_val, list_params):\n",
    "        \"\"\"\n",
    "        This function performs hyperparameter optimization using parallel computing.\n",
    "\n",
    "        :param x_train: array-like, shape (n_samples, n_features), input training data\n",
    "        :param y: array-like, shape (n_samples, ), target training values\n",
    "        :param x_val: array-like, shape (n_samples, n_features), input validation data\n",
    "        :param y_val: array-like, shape (n_samples, ), target validation values\n",
    "        :param list_params: list of dictionaries, where each dictionary contains a unique\n",
    "        combination of hyperparameters\n",
    "\n",
    "        :return: results: list of tuples, each containing the parameters, validation accuracy, and\n",
    "         run ID for one run of the model\n",
    "        \"\"\"\n",
    "        # Perform hyperparameter optimization\n",
    "        results = parallele_hyper_optim(in_num_workers=self.num_workers,\n",
    "                                        x_train=x_train,\n",
    "                                        y=y,\n",
    "                                        x_val=x_val,\n",
    "                                        y_val=y_val,\n",
    "                                        param_grid_outer=list_params,\n",
    "                                        in_framework=self.framework,\n",
    "                                        model_save_path=self.results_path,\n",
    "                                        in_mlflow_experiment_name=self.mlflow_experiment_id\n",
    "                                        )\n",
    "\n",
    "        return results\n",
    "\n",
    "    @staticmethod\n",
    "    def create_x_y(final_data_set: dict,\n",
    "                   in_key: str,\n",
    "                   in_features_to_extract: list,\n",
    "                   sample_freq: int,\n",
    "                   window_size: float,\n",
    "                   cutoff_frequency: int):\n",
    "        \"\"\"\n",
    "        Prepare the data by generating the x_data and y_hot_data arrays.\n",
    "        :param final_data_set: The final data set containing the input data.\n",
    "        :param in_key: The key to access the input data in the final_data_set.\n",
    "        :param in_features_to_extract: A list of features to extract from the data. These features\n",
    "                are extracted from final_data_set[in_key] and used as input (x_train).\n",
    "        :param sample_freq: The sample frequency in Hz.\n",
    "        :param window_size: The size of the window in seconds.\n",
    "        :param cutoff_frequency: The cutoff frequency for low-pass filtering.\n",
    "\n",
    "        Returns:\n",
    "            tuple: A tuple containing the x_train and y_train_oh arrays.\n",
    "        \"\"\"\n",
    "\n",
    "        sampling_rate = 1 / sample_freq\n",
    "        points_for_mean = int(window_size / sampling_rate)\n",
    "\n",
    "        dataset_features = []\n",
    "        for x in final_data_set[in_key]:\n",
    "            sp = SignalProcessor(input_signal=x,\n",
    "                                 in_filter=(LowPassFilter, {'cutoff_frequency': cutoff_frequency}),\n",
    "                                 in_window_size=points_for_mean,\n",
    "                                 in_stride=points_for_mean\n",
    "                                 )\n",
    "            sp.apply_filter(in_signal=sp.input_signal)\n",
    "            # sp.calibrate_data(in_signal=sp.output_signal)\n",
    "            features = sp.extract_windowed_features(in_signal=sp.output_signal)\n",
    "            dataset_features.append(features)\n",
    "\n",
    "        flattener = FeatureFlattener(final_data_set, in_features_to_extract)\n",
    "        new_feat = flattener.run(dataset_features)\n",
    "\n",
    "        final_data_set['new_feat'] = np.asarray(new_feat)\n",
    "\n",
    "        x_data = final_data_set['new_feat']\n",
    "        y_hot_data = np.asarray(final_data_set['y'])\n",
    "\n",
    "        return x_data, y_hot_data\n",
    "\n",
    "    @staticmethod\n",
    "    def read_data(in_path: str):\n",
    "        \"\"\"\n",
    "        read data\n",
    "        :param in_path: the path\n",
    "        \"\"\"\n",
    "        _, _, filenames = next(os.walk(in_path))\n",
    "        total_df = []\n",
    "        for file_x in filenames:\n",
    "            if file_x.endswith(\".log\"):\n",
    "                print(file_x[8:-4])\n",
    "                # Jutar preprocessed_path \\ file_x\n",
    "                path_to_read = os.path.join(in_path, file_x)\n",
    "                #  leer archivo path_to_read\n",
    "                input_df = pd.read_csv(path_to_read, delimiter=\"\\t\", header=None)\n",
    "                input_df[24] = file_x[8:-4]\n",
    "                total_df.append(input_df[[0, 1, 2, 5, 6, 7, 14, 15, 16, 23, 24]])\n",
    "        return pd.concat(total_df, axis=0)\n",
    "\n",
    "    @staticmethod\n",
    "    def _split_train_test(data: np.array, in_points_in_each_set: int):\n",
    "        \"\"\"\n",
    "        Splits the data into a training set and a test set.\n",
    "\n",
    "        Parameters:\n",
    "        data (np.array): The data to be split.\n",
    "        in_points_in_each_set (int): Number of points in each dataset.\n",
    "\n",
    "        Returns:\n",
    "        tuple: Contains arrays for training data and test data.\n",
    "        \"\"\"\n",
    "        # Convert 3a to image for each position\n",
    "        train_data = data[0:in_points_in_each_set]\n",
    "        test_data = data[in_points_in_each_set + 1: 2 * in_points_in_each_set + 1]\n",
    "        return train_data, test_data\n",
    "\n",
    "    def extract_data_during_activity(self,\n",
    "                                     in_num_of_users: int = 10,\n",
    "                                     in_dataset_dict: Dict = None,\n",
    "                                     in_accelerometer_position: List[str] = None) -> Dict:\n",
    "        \"\"\"\n",
    "        Extract data from physical activities periods (Label > 0) from dataset.\n",
    "        :param in_num_of_users: number of patients\n",
    "        :param in_dataset_dict: dictionary with data\n",
    "        :param in_accelerometer_position: position of accelerometer in the body\n",
    "        :return: dictionary final_data_set with train test data.\n",
    "        \"\"\"\n",
    "\n",
    "        data_keys = [f\"x_{key}\" for key in in_accelerometer_position]\n",
    "        final_data_set = {key: [] for key in ['y_label', 'y_onehot', 'user', *data_keys]}\n",
    "\n",
    "        list_of_classes = in_dataset_dict['y_num']\n",
    "        for label in np.unique(list_of_classes):\n",
    "            # Number of users\n",
    "            list_user = ['subject' + str(i + 1) for i in range(in_num_of_users)]\n",
    "            for user_i in list_user:\n",
    "                user_data_dict = extract_activity_data_from_users(\n",
    "                    user_i, label, in_dataset_dict, self.list_of_positions)\n",
    "\n",
    "                for key in in_accelerometer_position:\n",
    "                    final_data_set[f\"x_{key}\"].append(user_data_dict[f\"{key}_label\"])\n",
    "\n",
    "                final_data_set['y_label'].append(label)\n",
    "                final_data_set['y_onehot'].append(user_data_dict['y_onehot'][0])\n",
    "                final_data_set['user'].append(user_i)\n",
    "        return final_data_set\n",
    "\n",
    "    def split_data_for_train_test_portion_of_pa(self,\n",
    "                                                in_data: Dict,\n",
    "                                                in_points_in_each_set: int = int(1022 / 2),\n",
    "                                                in_accelerometer_position=None) -> Dict:\n",
    "        \"\"\"\n",
    "        Split data for training and test sets. in_points_in_each_set for training and\n",
    "        in_points_in_each_set for testing. Data is already a dict of lists where x[0] is a set of 3a\n",
    "        date that correspond to label y_label[0] and its one_hot_version (y_onehot[0]). Therefore,\n",
    "        the only variable that is processed (split) here is x\n",
    "        :param in_data: dictionary with data\n",
    "        :param in_points_in_each_set: int, number of points in the dataset\n",
    "        :param in_accelerometer_position: position of accelerometer in the body\n",
    "        :return: dictionary with train test data.\n",
    "        \"\"\"\n",
    "        # TODO: split is done 50% of an activity period trainning and 50% of the same activity for\n",
    "        #  training. Improve this\n",
    "\n",
    "        if in_accelerometer_position is None:\n",
    "            in_accelerometer_position = ['x_chest', 'x_ankle', 'x_right_arm']\n",
    "\n",
    "        train_test_keys = [f\"x_{key}_train\" for key in in_accelerometer_position] + \\\n",
    "                          [f\"x_{key}_test\" for key in in_accelerometer_position]\n",
    "        final_data_set = {key: [] for key in ['y_label', 'y_onehot', 'user', *train_test_keys]}\n",
    "\n",
    "        for key in in_accelerometer_position:\n",
    "            for data in in_data[f\"x_{key}\"]:\n",
    "                train_data, test_data = self._split_train_test(data, in_points_in_each_set)\n",
    "                final_data_set[f\"x_{key}_train\"].append(train_data)\n",
    "                final_data_set[f\"x_{key}_test\"].append(test_data)\n",
    "\n",
    "        # Label are the same training and test set\n",
    "        final_data_set['y_label'] = in_data['y_label']\n",
    "        final_data_set['y_onehot'] = in_data['y_onehot']\n",
    "        final_data_set['user'] = in_data['user']\n",
    "        final_data_set['y'] = final_data_set['y_onehot']\n",
    "\n",
    "        # And you have a list of keys that you want to keep:\n",
    "        base_keys = ['y', 'y_label', 'y_onehot', 'user']\n",
    "        keys_to_keep = base_keys + [f\"x_{key}_train\" for key in in_accelerometer_position]\n",
    "\n",
    "        # You can create a new dictionary with only these keys as follows:\n",
    "        data_set_train = {k: final_data_set[k] for k in keys_to_keep if k in final_data_set}\n",
    "\n",
    "        keys_to_keep = base_keys + [f\"x_{key}_test\" for key in in_accelerometer_position]\n",
    "\n",
    "        # You can create a new dictionary with only these keys as follows:\n",
    "        data_set_test = {k: final_data_set[k] for k in keys_to_keep if k in final_data_set}\n",
    "\n",
    "        return data_set_train, data_set_test\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    analysis = MHealtAnalysis()\n",
    "    analysis.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
